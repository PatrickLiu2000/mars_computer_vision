<!DOCTYPE html>
<html lang="en"><head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Class Project Fall 2020, Georgia Tech CS 4476 Devi Parikh</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>

<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name -->
<h1>CS 4476 Group Project</h1>
<span style="font-size: 20px; line-height: 1.5em;"><strong>Members: Winter Galyon, Patrick Liu, Connor Reitz, Joey Crawford</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Computer Vision Class Project, Fall 2020</span><br>
<span style="font-size: 18px; line-height: 1.5em;"> Georgia Tech CS 4476, Intructor: Devi Parikh</span>
<hr>

<!-- Goal -->
<h3>Abstract</h3>

Our project aims to identify where an input satellite image was taken relative to a known comprehensive satellite image. The case we hope to apply this to is an elevation map of mars provided by Google Mars. The input to our algorithm is an image of the surface of Mars and the output would be the location of that image relative to the elevation map we use as a reference as well as the orientation and size of the image relative to the Google Mars map, which could be used to identify the elevation the image would have been taken at if from a satellite.
<br><br>
<h3>Introduction</h3>
The problem we are solving is narrowly defined as locating the place where an image was taken from a satellite image of mars. This is applicable to real life space missions where a satellite may use this to find a location with accuracy, as inertial sensors often propagate error and there is not the benefit of GPS while in space. Using large high resolution images provided by the Google Mars project, we are using multiple approaches and contrasting them, ideally creating a combined algorithm with the redundancy necessary to accurately pinpoint a location in a variety of scenarios. There is no current computer vision approach to solving the problem of satellite localization, but the methods being used are similar to other image matching approaches.
<br></br>
The first approach is using a hough circle detector to find circles in a test image. These circles can then be compared to the circles in the map image to find a section with a similar feature signature to the test image. This is similar to a feature matching algorithm, with a focus on circles as the martian surface is covered in craters which make circle detection a uniquely effective feature detector.
<br></br>
The next approach is using a blob detector along with using SURF (speed up robust features) to perform object detection for the craters. We will then use this in combination with an SSD (single-shot multibox detector) to increase the accuracy of detecting these craters.
<h3>Teaser Figure</h3>
<img src="./images/teaser.jpeg" style="width:600px;height:500px;">
<!-- Approach -->
<h3>Approach</h3>
<ul>
  <li>Our approach will result in a confidence matrix of the possible locations for the center of the viewing window (the input image). This will be constructed using several approaches which will be weighted and combined into a single nxm matrix where n and m are the dimensions of the google mars source image. (we can orient the input image when we get to that by matching the average gradient direction of the input window and the Google Mars picture)</li>
  <li>One approach we will conduct is to find the center of a large number of the craters in the source image (elevation map from Google Mars) and store these as a nxm matrix wherein the center of each crater will be recorded as defined by a run of the hough circle detector (using gradients to decrease noise) on a set number of known radii. This will result in a number (R) of nxm matrices, one for each radius we choose to edge detect with.This algorithm will then be run on the input image and another R matrices will be made with a number of centers. This will then be randomly overlaid with the source image and a Gradient Descent algorithm with Simulated Annealing will be run to gradually converge on a local minimum (ideally the true minimum) for the distance to predicted crater centers. This will be run many times to prevent the location of local minima rather than the true minimum. The equation we want to optimise is the sum of distances between each center in the input to the closest centers in the source image. We will begin by writing the algorithm with a known window size for the input and known orientation (the same as the Google Mars orientation).</li>

  <li>We will also be implementing a version of Speeded-Up Robust Features (SURF) to map an image to a location on Mars. To break down my process, the algorithm will have three parts: interest point identification, local neighborhood description, and matching. Utilizing the integral image, the algorithm will filter and use a blob detector to detect interest points. Different scales are obtained by applying box filters of different sizes. Local orientation vectors are then calculated using Haar wavelet responses weighed by a Gaussian function to determine the dominant orientation. The descriptors from the image and Mars will be compared to determine matching pairs. </li>
  <li> Another technique we can use to perform feature detection and matching is by using SIFT (scale invariant feature transform), which is robust to rotation and scale. Initially to actually extract the features, we use a space-scale blob detector to identify the local maxima in the image. We then create a vector based on these extracted features using SIFT and use a thresholding along with euclidean distance ratios to feature match. </li>
  <li>One last technique we will experiment with (measuring accuracy and speed) is by using the BRIEF (binary robust independent elementary features) feature detection and matching method. This method matches keypoints between the test images and the original image by using binary feature vectors. The image will first be smoothed by a Gaussian kernel. Then, the algorithm will find keypoints in the image and create descriptors to encode a numerical “fingerprint” so we can tell each feature apart from another. This image will then take image patches around the keypoint pixels, convert them into binary feature vectors, and represent the objects for matching. </li>

</ul>

<br><br>
<!-- Results -->
<h3>Experiments and results</h3>
Our experiment is to compare a set of images of different sizes and orientations from random locations on the Google Mars ‘visible’ map with the full image. Knowing where the image is on the map, determine how accurately the program was able to locate where the image was at using the error calculation on the x and y location of the predicted location as well as the size and orientation. We will begin by knowing how large the window size is and that the orientation is north south (the same as the google mars source image). Then as we succeed we will make the algorithm more robust by using different orientations and gradually decreasing the window size of input images to prove greater efficacy.

For our success criteria, the program is expected to be slightly less accurate with smaller areas of the map, as there are less features that we can extract to define the image. Initially we will start off by demonstrating a brute-force approach, which will be very computationally expensive and take very long to run as a result. We will then contrast this brute-force approach with the other algorithms/optimizations that we found through our research and compare the computation time as well as accuracy to our other approaches that we present. To actually assess the runtime and accuracy, we plan on using the time library for Python. For computing accuracy, we plan on using several metrics such as SSD and RMSE to compare our approaches.
<h4>Hough Circle Detection Approach</h4>
To allow for feature matching the hough circle detector had to be calibrated for the mars images. Using two test images shown below, the sigma for the canny image detector and a range of possible circle sizes was determined. The optimal sigma was found to be 3 for the images we are testing with as it qualitatively displays rough circles which are representative of the actual image’s craters. This, however, may be further altered in the future so that the algorithm weighs which sigma gives the best possible set of edges depending on the quality of the image provided. However, there are limited datasets displaying the entirety of the martian surface so there is only minimal necessity for this algorithm to succeed on other datasets.
<br></br>
A range of circles from radius 10 to 90 pixels was found to encompass the majority of distinctive craters in the mars images. The hough circle detector will find only three craters in each image. This is because using three craters, the features found will be unique to the image being scanned. If only two were used, then any two craters could be matched to the image being compared given a rotation and dilation. With three, a unique triangle is made between the circles at each radius. The images below are the circles found at each radius on two test images. These images show that the algorithm is generally able to find three circles of various radii, and because circle detection is performed at 9 different radii the redundancy will allow the comparison between images to be robust so that a unique fingerprint is made of the images being searched for.
<br></br>
<h3>Test image 1</h3>
<img src= "./images/circle/rad10.png" style="width:300px;height:300px;">
<br></br>
<br></br>
<img src= "./images/circle/rad20.png" style="width:300px;height:300px;">
<br></br>
<br></br>
<img src= "./images/circle/rad30.png" style="width:300px;height:300px;">
<br></br>
<br></br>
<img src= "./images/circle/rad40.png" style="width:300px;height:300px;">
<br></br>
<br></br>
<img src= "./images/circle/rad50.png" style="width:300px;height:300px;">
<br></br>
<br></br>
<img src= "./images/circle/rad60.png" style="width:300px;height:300px;">
<br></br>
<br></br>
<img src= "./images/circle/rad70.png" style="width:300px;height:300px;">
<br></br>
<br></br>
<img src= "./images/circle/rad80.png" style="width:300px;height:300px;">
<br></br>
<br></br>
<img src= "./images/circle/rad90.png" style="width:300px;height:300px;">
<br></br>
<h3>Test image 2</h3>
<br></br>
<img src= "./images/circle/rad210.png" style="width:300px;height:300px;">
<br></br>
<br></br>
<img src= "./images/circle/rad220.png" style="width:300px;height:300px;">
<br></br>
<br></br>
<img src= "./images/circle/rad230.png" style="width:300px;height:300px;">
<br></br>
<br></br>
<img src= "./images/circle/rad240.png" style="width:300px;height:300px;">
<br></br>
<br></br>
<img src= "./images/circle/rad250.png" style="width:300px;height:300px;">
<br></br>
<br></br>
<img src= "./images/circle/rad260.png" style="width:300px;height:300px;">
<br></br>
<br></br>
<img src= "./images/circle/rad270.png" style="width:300px;height:300px;">
<br></br>
<br></br>
<img src= "./images/circle/rad280.png" style="width:300px;height:300px;">
<br></br>
<br></br>
<img src= "./images/circle/rad290.png" style="width:300px;height:300px;">
<br></br>
<h3>Test image 3</h3>
<br></br>
<img src= "./images/circle/rad310.png" style="width:300px;height:300px;">
<br></br>
<br></br>
<img src= "./images/circle/rad320.png" style="width:300px;height:300px;">
<br></br>
<br></br>
<img src= "./images/circle/rad330.png" style="width:300px;height:300px;">
<br></br>
<br></br>
<img src= "./images/circle/rad340.png" style="width:300px;height:300px;">
<br></br>
<br></br>
<img src= "./images/circle/rad350.png" style="width:300px;height:300px;">
<br></br>
<br></br>
<img src= "./images/circle/rad360.png" style="width:300px;height:300px;">
<br></br>
<br></br>
<img src= "./images/circle/rad370.png" style="width:300px;height:300px;">
<br></br>
<br></br>
<img src= "./images/circle/rad380.png" style="width:300px;height:300px;">
<br></br>
<br></br>
<img src= "./images/circle/rad390.png" style="width:300px;height:300px;">
<br></br>
The next step is to perform a calculation of chamfer distance. The locations of the circles’ centers were saved as .npy files, which were then fed into a function to calculate the minimum distance between the three circles in one image to three circles in a second image. This was also performed comparing a single image to itself, where the chamfer distance is expected to be zero, as the images are the same. The results for points in circles of radius 10 are shown in the table below.
<br></br>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-0lax"></th>
    <th class="tg-0lax">Image 1</th>
    <th class="tg-0lax">Image 2</th>
    <th class="tg-0lax">Image 3</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0lax">Image 1</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">154.30152681719923</td>
    <td class="tg-0lax">205.43739841789466</td>
  </tr>
  <tr>
    <td class="tg-0lax">Image 2</td>
    <td class="tg-0lax">154.30152681719923</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">173.83933364853746</td>
  </tr>
  <tr>
    <td class="tg-0lax">Image 3</td>
    <td class="tg-0lax">205.43739841789466</td>
    <td class="tg-0lax">173.83933364853746</td>
    <td class="tg-0lax">0</td>
  </tr>
</tbody>
</table>
Finally we applied the distance algorithm to a collection of images from the main map and were able to find which image is a match using the minimum chamfer distance. Ideally We would iterate through the entire image but this approach was found to be computationally intense, so in the interest of time and with a lack of large supercomputers (as would be used in a professional space expedition) the small scale test achieves the same goal of localizing an image using craters as features.
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-0pky"></th>
    <th class="tg-0pky">Image 1</th>
    <th class="tg-0pky">Image 2</th>
    <th class="tg-0pky">Image 3</th>
    <th class="tg-0lax">Image 4</th>
    <th class="tg-0lax">Image 5</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0pky">Test Image</td>
    <td class="tg-0pky">5</td>
    <td class="tg-0pky">153</td>
    <td class="tg-0pky">203</td>
    <td class="tg-0lax">922</td>
    <td class="tg-0lax">510</td>
  </tr>
</tbody>
</table>
<br></br>
The test results above show that this approach is able to successfully identify the location an image is taken from using only 3 circular image features at multiple radii (as it was trying to find image 1 and did so successfully). This result is promising due to the high degree of accuracy, but the concern would be that the computational time required to find the number of craters is too much to be feasible as a quick localization strategy, without the assistance of large computers. However, the intended client being agencies like NASA or companies with large budgets, this is not a primary concern.
<br></br>
This approach could also be improved. The goal was set in the beginning to find the height at which an image was taken from the martian surface, but this was proven to be very unreliable. This is because at different heights the crater’s distribution is not unique enough to give an accurate estimate in any way. Also, there are areas of the test images which do not have image data. This was worked around in the code by simply ignoring those sections, but it did result in areas with missing sections matching with the query image when the variable of height was added. As such this approach was quickly abandoned. However, if given more time to adapt the problem we believe it is possible to do height localization by incorporating other methods, perhaps using triangulation of shadows or finding key features like known crater centers and identifying those.
<br></br>
<h4> Blob Detector + SSD Approach </h4>
To preprocess the data, the raw images are done in a cylindrical projection, making viewing the entire map difficult:
<br><br>
<img src="./images/patrick/projection.png" style="width:300px;height:300px;">
<br><br>
We first reproject the map, correcting for distortions:
<br><br>
<img src="./images/patrick/reproject.png" style="width:300px;height:300px;">
<br><br>
We then split up this reprojected image into tiles for ease of processing and crater detection. For example:
<br><br>
<img src="./images/patrick/tiles.png" style="width:500px;height:250px;">
<br><br>
Now, I am using an object detector for each tile, specifically using a blob detector and will extract the SURF descriptors for each region, which I used an already existing implementation. I then use an estimator to determine whether or not the objects detected are craters, specifically using gradient boosted decision tree classifiers from sklearn. Currently I am still working on this, but here is a preliminary prediction:
<br><br>
<img src="./images/patrick/predicted_craters.png" style="width:300px;height:300px;">
<br><br>
Next steps for my approach include using Keras's implementation of a single-shot multibox detector(SSD).
<br><br>

<h4> BRIEF Approach </h4>
For the BRIEF approach, we used OpenCV to extract key points and descriptors from a plane image of Mars and a local region within the plane image. The key points were detected using STAR, and then we used OpenCV’s BriefDescriptorExtractor to get the descriptors from those key points. The find_and_match_brief method in BRIEF_approach.py uses brute force matching and takes in a number of matches to display in the results – showing all matches by default.

For this mid-project update, the BRIEF method was applied to one image and one local region to test out the method itself. In the final update, the method will be applied to various images with different local regions to test for speed and accuracy. For now, we can see the results of our BRIEF matching method:
(Images computed with all matches showing)

<img src="images/brief/all_matches.png">
<br>
a)  Local region within a large photo and its detected key points

<img src="images/brief/all_matches_matched.png">
<br>
b)  Local regions key points matched to regions in the plane (hard to see due to size of image)

<img src="images/brief/all_matches_zoom.png">
<br>
c)  Zoomed in plane to local area to better see local matches
<br>
<br>
<img src="images/brief/stats_update1.png">
<br>
<br>
d)  Statistics for the BRIEF approach (not including displaying the images, just the calculations themselves for detection and matching)
<br><br>
Using a relatively small window size in comparison to the overall image, 10.27 seconds for computing all the key points and matching them seems to be relatively fast. However, the method so far seems to lack accuracy. Out of 133 matches, there are not many that match to the location of the window I selected in the plane image. I believe this can be explained by 2 factors: the window size and the composition of the Mars surface. BRIEF doesn’t seem to be detecting features close to the edge of the local region, which can be seen in figure A. Any window size smaller actually was unable to detect any features for the image, so it is failing around the edges. For future updates, we will experiment more with larger local regions to see if accuracy is increased in matching. Secondly, the surface of Mars in this image contains many similar features; it is composed mainly of circular craters spread out around the entire surface. This may be the reason that it is detecting many matches outside of the desired region and could be an early indicator that detecting circles with the Hough transform may end up being the most accurate method of detection. In the next update, we will explore with preprocessing images before using the BRIEF approach to see if accuracy is increased. We will also compare the outputs of BRIEF to ORB (Oriented FAST and Rotated BRIEF).
<br><br>

<img src="images/brief/20_matches_local.png">
<br>
e)  local area with 20 best matches showing
<br>

<img src="images/brief/20_matches_zoom.png">
<br>
f)  local area matches in plane
<br><br>

Reiterating what was said before, accuracy seems to be low for these early attempts at using the BRIEF approach. Using different window sizes and preprocessing, we hope to increase the accuracy of this method.

<br><br>
<h3> Conclusions and Future Work </h3>

<h4> BRIEF Approach </h4>

As aforementioned, much more experimentation will take place with the basic framework of the BRIEF approach up and working. Different images of the surface of Mars, different local region window sizes, and preprocessing will all be experimented at different levels to attain a most accurate matching of regions to overall surface images of Mars. We will also compute processing times for when changing these parameters to measure for speed as well.

<h4> Blob Detector + SSD Approach </h4>
Given the predictions that the blob detector has made so far, the accuracies seem promising. Along with using an neural network, this will most likely yield the most accurate result out of all the approaches.

<h3>Resources</h3>
<ul>
  <li> <a href= "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.443.1732&rep=rep1&type=pdf River location
  http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.9601&rep=rep1&type=pdf"> Finding water bodies with neural nets</a> </li>
  <li> <a href="https://medium.com/descarteslabs-team/mapmaking-in-the-age-of-artificial-intelligence-da9e71be21d3">
  Finding features in satellite images</a> </li>
  <li> <a href="https://www.sciencedirect.com/science/article/pii/S2468232216300555">
  Location using chamfer distance </a> </li>
  <li> <a href="https://www.google.com/mars/">Mars elevation map </a> </li>
  <li> <a href="https://link.springer.com/chapter/10.1007/11744023_32"> SURF: Speeded Up Robust Features </a> </li>
  <li> <a href="https://towardsdatascience.com/sift-scale-invariant-feature-transform-c7233dc60f37"> SIFT </a> </li>
  <li> <a href="https://docs.python.org/3/library/time.html">Python time documentation </a> </li>
  <li> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0032063309000956">Automatic detection of sub-km craters in high resolution planetary images</a> </li>
  <li> <a href="https://docs.opencv.org/master/dc/dc3/tutorial_py_matcher.html">Matching descriptors in OpenCV</a> </li>
  <li> <a href="https://docs.opencv.org/master/dc/d7d/tutorial_py_brief.html">Implementing BRIEF in OpenCV </a> </li>
</ul>
<br><br>
  <hr>
  <footer>
  <p>© Patrick Liu</p>
  </footer>
</div>
</div>

<br><br>

</body></html>
